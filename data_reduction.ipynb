{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of the observations\n",
    "\n",
    "- Florian and Rainer stood a couple of meters apart both observing the ISS transit\n",
    "- Rainer used a 100m APO on an AltAz mount along with as EOS700D camera\n",
    "- Florian used the C8 Cassegrain telescope along with an EOS750D camera\n",
    "\n",
    "The following parameters are important for the analysis later as calculated for the midpoint between both observation sites:\n",
    "\n",
    "\n",
    "Fri 2023-08-18, 12:03:52.73  •  Sun transit\n",
    "ISS angular size: 49.70″; distance: 555.98 km\n",
    "Angular separation: 4.0′; azimuth: 154.5°; altitude: 48.4°\n",
    "Center line distance: 0.80 km; visibility path width: 6.23 km\n",
    "Transit duration: 0.74 s; transit chord length: 30.6′\n",
    "\n",
    "R.A.: 09h 50m; Dec: +13° 10′; parallactic angle: 15.4°\n",
    "ISS velocity: 41.6 ′/s (angular); 6.73 km/s (transverse)\n",
    "ISS velocity: 3.06 km/s (radial); 7.39 km/s (total);\n",
    "Direction of motion relative to zenith: 124.8°\n",
    "Sun angular size: 31.6′; 38.2 times larger than the ISS\n",
    "\n",
    "## Video facts:\n",
    "Rainer' Video:\n",
    "The ISS passage starts at roughly 02:08\n",
    "\n",
    "Florian's Video:\n",
    "The ISS passage starts at roughly 01:09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76 frames extracted and saved in rainer_frames\n"
     ]
    }
   ],
   "source": [
    "## Convert the video files into single, stckable images\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "def extract_frames(video_path, output_folder):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Check if the video file was opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file\")\n",
    "        return\n",
    "    \n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    frame_count = 0\n",
    "    \n",
    "    while True:\n",
    "        # Read a frame from the video\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Break the loop if we have reached the end of the video\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Save the frame as an image\n",
    "        frame_filename = os.path.join(output_folder, f\"frame_{frame_count:04d}.jpg\")\n",
    "        cv2.imwrite(frame_filename, frame)\n",
    "        \n",
    "        frame_count += 1\n",
    "    \n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "    \n",
    "    print(f\"{frame_count} frames extracted and saved in {output_folder}\")\n",
    "\n",
    "\n",
    "video_path = \"rainer_crop.mp4\"  # Replace with your input video file\n",
    "output_folder = \"rainer_frames\"  # Replace with the desired output folder name\n",
    "\n",
    "extract_frames(video_path, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Align all the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 22/22 [00:03<00:00,  6.83it/s]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "in_image  = \"rainer_frames/\"\n",
    "reference = \"rainer_frames/frame_0029.jpg\"\n",
    "\n",
    "counter = 0\n",
    "for image in tqdm(glob(in_image + \"*.jpg\")):\n",
    "    # Open the image files.\n",
    "    img1_color = cv2.imread(image) # Image to be aligned.\n",
    "    img2_color = cv2.imread(reference) # Reference image.\n",
    "    \n",
    "    # Convert to grayscale.\n",
    "    img1 = cv2.cvtColor(img1_color, cv2.COLOR_BGR2GRAY)\n",
    "    img2 = cv2.cvtColor(img2_color, cv2.COLOR_BGR2GRAY)\n",
    "    height, width = img2.shape\n",
    "    \n",
    "    # Create ORB detector with 5000 features.\n",
    "    orb_detector = cv2.ORB_create(5000)\n",
    "    \n",
    "    # Find keypoints and descriptors.\n",
    "    # The first arg is the image, second arg is the mask\n",
    "    # (which is not required in this case).\n",
    "    kp1, d1 = orb_detector.detectAndCompute(img1, None)\n",
    "    kp2, d2 = orb_detector.detectAndCompute(img2, None)\n",
    "    \n",
    "    # Match features between the two images.\n",
    "    # We create a Brute Force matcher with\n",
    "    # Hamming distance as measurement mode.\n",
    "    matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck = True)\n",
    "    \n",
    "    # Match the two sets of descriptors.\n",
    "    matches = matcher.match(d1, d2)\n",
    "    \n",
    "    # Sort matches on the basis of their Hamming distance.\n",
    "    matches = list(matches)\n",
    "    matches.sort(key = lambda x: x.distance)\n",
    "    \n",
    "    # Take the top 90 % matches forward.\n",
    "    matches = matches[:int(len(matches)*0.9)]\n",
    "    no_of_matches = len(matches)\n",
    "    \n",
    "    # Define empty matrices of shape no_of_matches * 2.\n",
    "    p1 = np.zeros((no_of_matches, 2))\n",
    "    p2 = np.zeros((no_of_matches, 2))\n",
    "    \n",
    "    for i in range(len(matches)):\n",
    "        p1[i, :] = kp1[matches[i].queryIdx].pt\n",
    "        p2[i, :] = kp2[matches[i].trainIdx].pt\n",
    "    \n",
    "    # Find the homography matrix.\n",
    "    homography, mask = cv2.findHomography(p1, p2, cv2.RANSAC)\n",
    "    \n",
    "    # Use this matrix to transform the\n",
    "    # colored image wrt the reference image.\n",
    "    transformed_img = cv2.warpPerspective(img1_color,\n",
    "    \t\t\t\t\thomography, (width, height))\n",
    "    \n",
    "    # Save the output.\n",
    "    cv2.imwrite(in_image + \"%i_align.jpg\" % counter, transformed_img)\n",
    "    counter += 1\n",
    "    #cv2.imwrite(in_image[:-4] + \"_align.jpg\", transformed_img)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
